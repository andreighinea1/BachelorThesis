{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Load SEED Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a29db2a0b135a13b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load RAW EEG",
   "id": "24699bd8dcadd704"
  },
  {
   "cell_type": "code",
   "source": [
    "from dataset_processing.seed_dataset_loader import SeedDatasetLoader\n",
    "\n",
    "sampling_frequency = 200  # 200 Hz\n",
    "\n",
    "_loader = SeedDatasetLoader(fs=sampling_frequency)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f28dcf2b965f8640",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "labels = _loader.get_labels()\n",
    "labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca4d22bed4a15e46",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "channel_order = _loader.get_channel_order()\n",
    "channel_order"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8b0f267cb93aacf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "_eeg_data_df = _loader.get_eeg_data_df()",
   "id": "71c84272baaf52fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "_loader.plot_random_eeg()",
   "metadata": {
    "collapsed": false
   },
   "id": "e227135b42d3ad7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "del _loader",
   "id": "af4511eb8ab884d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Data Augmentation",
   "metadata": {
    "collapsed": false
   },
   "id": "3405db51e6dde34e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dataset_processing.eeg_augmentation import EEGAugmentation\n",
    "\n",
    "_augmentor = EEGAugmentation(_eeg_data_df)\n",
    "_augmented_df = _augmentor.augment_data()\n",
    "del _augmentor, _eeg_data_df"
   ],
   "id": "cb020fa64271167f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset Loader",
   "id": "9999e0a7f8511bc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataset_processing.eeg_dataset import EEGDataset\n",
    "\n",
    "# From the paper\n",
    "pretraining_batch_size = 256\n",
    "\n",
    "_dataset = EEGDataset(_augmented_df)\n",
    "data_loader = DataLoader(_dataset, batch_size=pretraining_batch_size, shuffle=True)\n",
    "del _augmented_df, _dataset"
   ],
   "id": "18f58fe7d5fb9e23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pre-Training",
   "id": "ad9af7bc72ac020d"
  },
  {
   "cell_type": "code",
   "source": [
    "from model.encoders import TimeFrequencyEncoder, CrossSpaceProjector\n",
    "from model.loss import NTXentLoss\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ebf017b23d22c9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "pretraining_model_save_dir = \"model_params/pretraining\"\n",
    "finetuning_model_save_dir = \"model_params/finetuning\"\n",
    "if not os.path.exists(pretraining_model_save_dir):\n",
    "    os.makedirs(pretraining_model_save_dir)\n",
    "if not os.path.exists(finetuning_model_save_dir):\n",
    "    os.makedirs(finetuning_model_save_dir)"
   ],
   "id": "42a63bb2d83b1144",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Parameters from the paper\n",
    "pretraining_epochs = 10  # Set to 10 for initial tests\n",
    "# pretraining_epochs = 1000\n",
    "pretraining_lr = 3e-4\n",
    "l2_norm_penalty = 3e-4\n",
    "alpha = 0.2\n",
    "beta = 1.0\n",
    "pretraining_temperature = 0.05\n",
    "\n",
    "encoders_output_dim = 200\n",
    "projectors_output_dim = 128"
   ],
   "id": "5d98d475d18c3099",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_layers = 2\n",
    "nhead = 8\n",
    "\n",
    "# Initialize models\n",
    "ET = TimeFrequencyEncoder(\n",
    "    input_dim=sampling_frequency,\n",
    "    output_dim=encoders_output_dim,\n",
    "    num_layers=num_layers,\n",
    "    nhead=nhead,\n",
    ").to(device)\n",
    "EF = TimeFrequencyEncoder(\n",
    "    input_dim=sampling_frequency,\n",
    "    output_dim=encoders_output_dim,\n",
    "    num_layers=num_layers,\n",
    "    nhead=nhead,\n",
    ").to(device)\n",
    "PT = CrossSpaceProjector(\n",
    "    input_dim=encoders_output_dim,\n",
    "    output_dim=projectors_output_dim,\n",
    ").to(device)\n",
    "PF = CrossSpaceProjector(\n",
    "    input_dim=encoders_output_dim,\n",
    "    output_dim=projectors_output_dim,\n",
    ").to(device)\n",
    "\n",
    "# Define optimizers with L2 penalty\n",
    "optimizer = optim.Adam(\n",
    "    list(ET.parameters()) + list(EF.parameters()) + list(PT.parameters()) + list(PF.parameters()),\n",
    "    lr=pretraining_lr,\n",
    "    weight_decay=l2_norm_penalty  # L2-norm penalty coefficient\n",
    ")\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer,\n",
    "    mode='min',\n",
    "    factor=0.1,\n",
    "    patience=10,\n",
    ")\n",
    "\n",
    "nt_xent_calculator = NTXentLoss(temperature=pretraining_temperature)\n",
    "\n",
    "# Pre-training loop\n",
    "for epoch in range(1, pretraining_epochs + 1):\n",
    "    pbar = tqdm(data_loader, desc=f\"Epoch {epoch}\", leave=False)\n",
    "    epoch_loss = 0\n",
    "    for xT, xT_augmented, xF, xF_augmented in pbar:\n",
    "        # Move batches of data to the `device`\n",
    "        xT = xT.to(device)\n",
    "        xT_augmented = xT_augmented.to(device)\n",
    "        xF = xF.to(device)\n",
    "        xF_augmented = xF_augmented.to(device)\n",
    "\n",
    "        # Reset the optimizers\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Time Domain Contrastive Learning\n",
    "        hT = ET(xT)  # Encode time data\n",
    "        hT_augmented = ET(xT_augmented)  # Encode augmented time data\n",
    "        LT = nt_xent_calculator.calculate_loss(  # Calculate the time-based contrastive loss LT in Eq. 1\n",
    "            hT,\n",
    "            hT_augmented\n",
    "        )\n",
    "\n",
    "        # Frequency Domain Contrastive Learning\n",
    "        hF = EF(xF)  # Encode frequency data\n",
    "        hF_augmented = EF(xF_augmented)  # Encode augmented frequency data\n",
    "        LF = nt_xent_calculator.calculate_loss(  # Calculate the frequency-based contrastive loss LF in Eq. 2\n",
    "            hF,\n",
    "            hF_augmented\n",
    "        )\n",
    "\n",
    "        # Time-Frequency Domain Contrastive Learning\n",
    "        zT = PT(hT)  # Project into shared latent space\n",
    "        zF = PF(hF)  # Project into shared latent space\n",
    "        LA = nt_xent_calculator.calculate_loss(  # Calculate the alignment loss LA in Eq. 3\n",
    "            zT,\n",
    "            zF\n",
    "        )\n",
    "\n",
    "        # Compute total loss\n",
    "        L = alpha * (LT + LF) + beta * LA\n",
    "        epoch_loss += L.item()\n",
    "\n",
    "        # Backpropagation\n",
    "        L.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update tqdm progress bar with the current loss\n",
    "        pbar.set_description_str(f\"Epoch {epoch}, Loss: {L.item():.4f}\")\n",
    "\n",
    "    # Step the scheduler based on the epoch loss\n",
    "    scheduler.step(epoch_loss)\n",
    "\n",
    "    # Save the model every 10 epochs and at the last epoch\n",
    "    if epoch % 10 == 0 or epoch == pretraining_epochs:\n",
    "        model_dicts = {\n",
    "            \"ET_state_dict\": ET.state_dict(),\n",
    "            \"EF_state_dict\": EF.state_dict(),\n",
    "            \"PT_state_dict\": PT.state_dict(),\n",
    "            \"PF_state_dict\": PF.state_dict(),\n",
    "        }\n",
    "        torch.save(model_dicts, os.path.join(pretraining_model_save_dir, f\"pretrained_model__epoch_{epoch}.pt\"))\n",
    "        print(f\"Saved model at epoch {epoch}\")\n",
    "\n",
    "    print(f\"Epoch {epoch}, Average Loss: {epoch_loss / len(data_loader):.4f}, Learning Rate: {scheduler.get_last_lr():.6f}\")"
   ],
   "id": "1d3b418678187f73",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "collapsed": false
   },
   "id": "3d6e9ae795b9605b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dae047e098f4c9f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Ideas",
   "id": "8099d554347b0eab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do a correlation matrix between the channels of the EEG signals.\n",
    "Then when doing the joint whatever model, use the \"distances\" between the channels (like the hamming distance but not really), as a \"weight\" for training the joining etc.\n",
    "\n",
    "Or maybe just output something that could show each channel's contribution towards the final emotion prediction."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1dddb3704166f6cd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
