{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Try to use these to fix outputs issue:\n",
    "\n",
    "https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "https://notebook.community/lifeinoppo/littlefishlet-scode/RES/REF/python_sourcecode/ipython-master/examples/IPython%20Kernel/Capturing%20Output"
   ],
   "id": "206fd0459d5c59b4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TODO: Add to references for using dgl library\n",
    "\n",
    "@article{wang2019dgl,\n",
    "    title={Deep Graph Library: A Graph-Centric, Highly-Performant Package for Graph Neural Networks},\n",
    "    author={Minjie Wang and Da Zheng and Zihao Ye and Quan Gan and Mufei Li and Xiang Song and Jinjing Zhou and Chao Ma and Lingfan Yu and Yu Gai and Tianjun Xiao and Tong He and George Karypis and Jinyang Li and Zheng Zhang},\n",
    "    year={2019},\n",
    "    journal={arXiv preprint arXiv:1909.01315}\n",
    "}"
   ],
   "id": "a50411dc88a41020"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TODO: Reference to paper:\n",
    "https://arxiv.org/abs/1606.09375"
   ],
   "id": "418ca73dc891345"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "conda install -c dglteam/label/th23_cu121 dgl\n",
    "\n",
    "conda install -c pytorch torchdata\n",
    "\n",
    "conda install pydantic -c conda-forge"
   ],
   "id": "673141019671f7d0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Common libs",
   "id": "3e0f4b0cf1c39ceb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# noinspection PyUnresolvedReferences\n",
    "from importlib import reload"
   ],
   "id": "9acb8a1fbeb40ff5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load SEED Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a29db2a0b135a13b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load RAW EEG",
   "id": "24699bd8dcadd704"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import sys\n",
    "\n",
    "from dataset_processing.seed_dataset_loader import SeedDatasetLoader\n",
    "\n",
    "sampling_frequency = 200  # 200 Hz\n",
    "\n",
    "_loader = SeedDatasetLoader(fs=sampling_frequency)"
   ],
   "id": "f28dcf2b965f8640",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "labels = _loader.get_labels()\n",
    "labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca4d22bed4a15e46",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "channel_order = _loader.get_channel_order()\n",
    "channel_order"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8b0f267cb93aacf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "_eeg_data_df = _loader.get_eeg_data_df()",
   "id": "71c84272baaf52fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "_loader.plot_random_eeg()",
   "metadata": {
    "collapsed": false
   },
   "id": "e227135b42d3ad7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "del _loader",
   "id": "af4511eb8ab884d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Data Augmentation",
   "metadata": {
    "collapsed": false
   },
   "id": "3405db51e6dde34e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dataset_processing.eeg_augmentation import EEGAugmentation\n",
    "\n",
    "_augmentor = EEGAugmentation(_eeg_data_df)\n",
    "_augmented_df = _augmentor.augment_data()\n",
    "del _augmentor, _eeg_data_df"
   ],
   "id": "cb020fa64271167f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Pre-Training",
   "id": "ad9af7bc72ac020d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataset_processing.eeg_dataset import EEGDataset\n",
    "from model.pre_training.do_pre_training import PreTraining\n",
    "\n",
    "# From the paper\n",
    "pretraining_batch_size = 256\n",
    "pretraining_epochs = 1000"
   ],
   "id": "c370fe3af6a91b1a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Add in bachelor thesis how `num_workers` was chosen with code from w_testing_values notebook\n",
    "\n",
    "# Custom cleanup function, useful when using the dataloader too much,\n",
    "# as it's bugged and needs manual cleaning (because of Jupyter Notebook)\n",
    "def cleanup_data_loader(loader):\n",
    "    # noinspection PyProtectedMember\n",
    "    if loader._iterator is not None:\n",
    "        # noinspection PyProtectedMember\n",
    "        loader._iterator._shutdown_workers()"
   ],
   "id": "1cfd0859dba8d07b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset Loader",
   "id": "116d350633bb880f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_workers = 5\n",
    "\n",
    "pretraining_data_loader = DataLoader(\n",
    "    EEGDataset(_augmented_df),\n",
    "    batch_size=pretraining_batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "\n",
    "    num_workers=num_workers,\n",
    "    prefetch_factor=2,  # Default: 2 for `num_workers` > 0  # TODO: Maybe set to 3-4\n",
    ")\n",
    "# del _augmented_df"
   ],
   "id": "ced1c49f14fc7150",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Do the pre-training",
   "id": "bb188c0dbcb163af"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import gc\n",
    "\n",
    "print(f\"Garbage collector: collected {gc.collect()} objects.\")"
   ],
   "id": "a376e1820aa3b4ae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: Use for simplified training: https://pytorch-ignite.ai/tutorials/beginner/01-getting-started/",
   "id": "a88a1da60cfc3f7f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "print(f\"Starting pre-training with {num_workers} workers loading the dataset\")\n",
    "\n",
    "try:\n",
    "    pretraining_model_trainer = PreTraining(\n",
    "        data_loader=pretraining_data_loader,\n",
    "        sampling_frequency=sampling_frequency,\n",
    "        pretraining_model_save_dir=\"model_params/pretraining\",\n",
    "        scheduler_patience=50,\n",
    "        early_stopping_patience=100,\n",
    "        # epochs=pretraining_epochs,\n",
    "        epochs=2000,\n",
    "    )\n",
    "    pretraining_model_trainer.train()\n",
    "except Exception as e:\n",
    "    print(e, file=sys.stderr)\n",
    "\n",
    "# cleanup_data_loader(pretraining_data_loader)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ebf017b23d22c9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir runs"
   ],
   "id": "a0441a2dd06e00a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Fine-Tuning",
   "id": "d52e28a9f9218d03"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataset_processing.eeg_dataset import EEGDataset\n",
    "from model.fine_tuning.do_fine_tuning import FineTuning\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# From the paper\n",
    "finetuning_batch_size = 128\n",
    "finetuning_epochs = 20"
   ],
   "id": "e64c6783f5db231d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset Loader",
   "id": "9ee25fc044cb4938"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "num_workers = 5\n",
    "\n",
    "# Customizable percentages\n",
    "train_percentage = 0.7\n",
    "eval_percentage = 0.3\n",
    "\n",
    "# \"Verdict\" is the column name representing the verdict class\n",
    "train_df_list = []\n",
    "eval_df_list = []\n",
    "\n",
    "# Ensure balanced datasets by splitting for each verdict class\n",
    "for verdict in _augmented_df[\"Verdict\"].unique():\n",
    "    verdict_df = _augmented_df[_augmented_df[\"Verdict\"] == verdict]\n",
    "    train_df, eval_df = train_test_split(\n",
    "        verdict_df,\n",
    "        train_size=train_percentage,\n",
    "        random_state=42,\n",
    "        stratify=verdict_df[\"Verdict\"]\n",
    "    )\n",
    "    train_df_list.append(train_df)\n",
    "    eval_df_list.append(eval_df)\n",
    "\n",
    "    # Print the sizes for each verdict\n",
    "    print(f\"Verdict: {verdict}\")\n",
    "    print(f\"TOTAL size: {len(train_df) + len(eval_df)}\")\n",
    "    print(f\"- Train size: {len(train_df)}\")\n",
    "    print(f\"- Eval  size: {len(eval_df)}\\n\")\n",
    "\n",
    "# Combine the balanced splits back into training and evaluation datasets\n",
    "train_df = pd.concat(train_df_list).reset_index(drop=True)\n",
    "eval_df = pd.concat(eval_df_list).reset_index(drop=True)\n",
    "\n",
    "# Create DataLoaders\n",
    "finetuning_data_loader = DataLoader(\n",
    "    EEGDataset(train_df),\n",
    "    batch_size=finetuning_batch_size,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    num_workers=num_workers,\n",
    "    prefetch_factor=2,\n",
    ")\n",
    "\n",
    "finetuning_data_loader_eval = DataLoader(\n",
    "    EEGDataset(eval_df),\n",
    "    batch_size=finetuning_batch_size,\n",
    "    shuffle=False,\n",
    "    pin_memory=True,\n",
    "    persistent_workers=True,\n",
    "    num_workers=num_workers,\n",
    "    prefetch_factor=2,\n",
    ")"
   ],
   "id": "a6e78ed04b8755b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Do the fine-tuning",
   "id": "4b86e0b0a5e34884"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pretraining_model = PreTraining(\n",
    "    data_loader=None,\n",
    "    sampling_frequency=sampling_frequency,\n",
    "    pretraining_model_save_dir=\"model_params/pretraining\",\n",
    "    scheduler_patience=50,\n",
    "    early_stopping_patience=100,\n",
    "    epochs=2000,\n",
    "    to_train=False,\n",
    ")\n",
    "pretraining_model.load_model(2000)"
   ],
   "id": "c52e68a6e365e227",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "try:\n",
    "    finetuning = FineTuning(\n",
    "        data_loader=finetuning_data_loader,\n",
    "        data_loader_eval=finetuning_data_loader_eval,\n",
    "        sampling_frequency=sampling_frequency,\n",
    "        num_classes=3,\n",
    "\n",
    "        ET=pretraining_model.ET,\n",
    "        EF=pretraining_model.EF,\n",
    "        PT=pretraining_model.PT,\n",
    "        PF=pretraining_model.PF,\n",
    "\n",
    "        finetuning_model_save_dir=\"model_params/finetuning\",\n",
    "        # epochs=finetuning_epochs,\n",
    "        epochs=20,\n",
    "    )\n",
    "    finetuning.train()\n",
    "except Exception as e:\n",
    "    print(e, file=sys.stderr)"
   ],
   "id": "4eb9f66758227bd3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "finetuning_model = FineTuning(\n",
    "    data_loader=finetuning_data_loader,\n",
    "    data_loader_eval=finetuning_data_loader_eval,\n",
    "    sampling_frequency=sampling_frequency,\n",
    "    num_classes=3,\n",
    "\n",
    "    ET=pretraining_model.ET,\n",
    "    EF=pretraining_model.EF,\n",
    "    PT=pretraining_model.PT,\n",
    "    PF=pretraining_model.PF,\n",
    "\n",
    "    finetuning_model_save_dir=\"model_params/finetuning\",\n",
    "    to_train=False,\n",
    ")\n",
    "finetuning_model.load_model(20)\n",
    "\n",
    "eval_accuracy, avg_eval_loss = finetuning_model.do_eval_epoch()\n",
    "print(\n",
    "    f\"Eval Loss: {avg_eval_loss:.4f}, \"\n",
    "    f\"Eval Accuracy: {eval_accuracy:.4f}\"\n",
    ")"
   ],
   "id": "82b99e44275bb153"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Ideas",
   "id": "8099d554347b0eab"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do a correlation matrix between the channels of the EEG signals.\n",
    "Then when doing the joint whatever model, use the \"distances\" between the channels (like the hamming distance but not really), as a \"weight\" for training the joining etc.\n",
    "\n",
    "Or maybe just output something that could show each channel's contribution towards the final emotion prediction."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1dddb3704166f6cd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
