{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Load SEED Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a29db2a0b135a13b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Load RAW EEG",
   "id": "24699bd8dcadd704"
  },
  {
   "cell_type": "code",
   "source": [
    "from dataset_processing.seed_dataset_loader import SeedDatasetLoader\n",
    "\n",
    "_loader = SeedDatasetLoader()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f28dcf2b965f8640",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "labels = _loader.get_labels()\n",
    "labels"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ca4d22bed4a15e46",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "channel_order = _loader.get_channel_order()\n",
    "channel_order"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8b0f267cb93aacf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "_eeg_data_df = _loader.get_eeg_data_df()",
   "id": "71c84272baaf52fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "_loader.plot_random_eeg()",
   "metadata": {
    "collapsed": false
   },
   "id": "e227135b42d3ad7c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "del _loader",
   "id": "af4511eb8ab884d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "### Data Augmentation",
   "metadata": {
    "collapsed": false
   },
   "id": "3405db51e6dde34e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from dataset_processing.eeg_augmentation import EEGAugmentation\n",
    "\n",
    "_augmentor = EEGAugmentation(_eeg_data_df)\n",
    "_augmented_df = _augmentor.augment_data()\n",
    "del _augmentor, _eeg_data_df"
   ],
   "id": "cb020fa64271167f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Dataset Loader",
   "id": "9999e0a7f8511bc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from dataset_processing.eeg_dataset import EEGDataset\n",
    "\n",
    "_dataset = EEGDataset(_augmented_df)\n",
    "data_loader = DataLoader(_dataset, batch_size=32, shuffle=True)\n",
    "del _augmented_df, _dataset"
   ],
   "id": "18f58fe7d5fb9e23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training",
   "id": "ad9af7bc72ac020d"
  },
  {
   "cell_type": "code",
   "source": [
    "from model.encoders import TimeFrequencyEncoder, CrossSpaceProjector\n",
    "from model.loss import NTXentLoss\n",
    "import torch.optim as optim\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ebf017b23d22c9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "epochs = 10\n",
    "lr = 0.001\n",
    "\n",
    "alpha = 0.2\n",
    "beta = 1.0\n",
    "temperature = 0.05"
   ],
   "id": "83a0c421a56adab2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize models\n",
    "ET = TimeFrequencyEncoder().to(device)\n",
    "EF = TimeFrequencyEncoder().to(device)\n",
    "PT = CrossSpaceProjector().to(device)\n",
    "PF = CrossSpaceProjector().to(device)\n",
    "\n",
    "# Define optimizers\n",
    "optimizer_ET = optim.Adam(ET.parameters(), lr=lr)\n",
    "optimizer_EF = optim.Adam(EF.parameters(), lr=lr)\n",
    "optimizer_PT = optim.Adam(PT.parameters(), lr=lr)\n",
    "optimizer_PF = optim.Adam(PF.parameters(), lr=lr)\n",
    "\n",
    "nt_xent_calculator = NTXentLoss(temperature=temperature)\n",
    "\n",
    "# Pre-training loop\n",
    "for epoch in range(epochs):\n",
    "    for xT, xT_augmented, xF, xF_augmented in data_loader:\n",
    "        # Move batches of data to the `device`\n",
    "        xT = xT.to(device)\n",
    "        xT_augmented = xT_augmented.to(device)\n",
    "        xF = xF.to(device)\n",
    "        xF_augmented = xF_augmented.to(device)\n",
    "\n",
    "        # Reset the optimizers\n",
    "        optimizer_ET.zero_grad()\n",
    "        optimizer_EF.zero_grad()\n",
    "        optimizer_PT.zero_grad()\n",
    "        optimizer_PF.zero_grad()\n",
    "\n",
    "        # Time Domain Contrastive Learning\n",
    "        hT = ET(xT)  # Encode time data\n",
    "        hT_augmented = ET(xT_augmented)  # Encode augmented time data\n",
    "        LT = nt_xent_calculator.calculate_loss(  # Calculate the time-based contrastive loss LT in Eq. 1\n",
    "            hT,\n",
    "            hT_augmented\n",
    "        )\n",
    "\n",
    "        # Frequency Domain Contrastive Learning\n",
    "        hF = EF(xF)  # Encode frequency data\n",
    "        hF_augmented = EF(xF_augmented)  # Encode augmented frequency data\n",
    "        LF = nt_xent_calculator.calculate_loss(  # Calculate the frequency-based contrastive loss LF in Eq. 2\n",
    "            hF,\n",
    "            hF_augmented\n",
    "        )\n",
    "\n",
    "        # Time-Frequency Domain Contrastive Learning\n",
    "        zT = PT(hT)  # Project into shared latent space\n",
    "        zF = PF(hF)  # Project into shared latent space\n",
    "        LA = nt_xent_calculator.calculate_loss(  # Calculate the alignment loss LA in Eq. 3\n",
    "            zT,\n",
    "            zF\n",
    "        )\n",
    "\n",
    "        # Compute total loss\n",
    "        L = alpha * (LT + LF) + beta * LA\n",
    "\n",
    "        # Backpropagation\n",
    "        L.backward()\n",
    "        optimizer_ET.step()\n",
    "        optimizer_EF.step()\n",
    "        optimizer_PT.step()\n",
    "        optimizer_PF.step()\n",
    "\n",
    "        print(f\"Epoch {epoch}, Loss: {L.item()}\")"
   ],
   "id": "1d3b418678187f73",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "collapsed": false
   },
   "id": "3d6e9ae795b9605b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dae047e098f4c9f4",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do a correlation matrix between the channels of the EEG signals.\n",
    "Then when doing the joint whatever model, use the \"distances\" between the channels (like the hamming distance but not really), as a \"weight\" for training the joining etc."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1dddb3704166f6cd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
